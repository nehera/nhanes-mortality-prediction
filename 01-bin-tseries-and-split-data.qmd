---
title: "Binning NHANES Activity Time Series and Splitting Mortality, Covariates, and Activity Data into Train, Valid, Test Sets"
author: "Aidan Neher"
format: html
editor: visual
---

# NHANES Tutorial

## Introduction

This tutorial illustrates how to start working with data collected from 2003-2004 ("C") and 2005-2006 ("D") waves of National Health and Nutrition Examination Survey (NHANES), one of the largest study conducted by CDC to assess the health and nutritional status of US population. Specifically, we focus on (1) activity count data collected from accelerometers of hip-worn devices (ActiGraph AM-7164), (2) mortality data collected from National Death Index and linked to NHANES, and (3) demographic, survey design, lifestyle, and comorbiditiy data. These data have been processed and stored in the `rnhanesdata` package. The processing pipeline can be found [here](https://andrew-leroux.github.io/rnhanesdata/articles/create-all-processed-data.html).

In this tutorial, we assume that the `rnhanesdata` package has been successfully installed in your working environment. The package installation tutorial can be found [here](https://github.com/andrew-leroux/rnhanesdata). The name of the processed data are listed as follows:

-   Activity count data: PAXINTEN_C, PAXINTEN_D, FLAG_C, FLAG_D.
-   Mortality data: Mortality_2015_C, Mortality_2015_D.
-   Demographic, survey design, lifestyle, and comorbiditiy data: Covariate_C, Covariate_D.

For activity count data, "PAXINTEN\_\*" are matrices of count values and "FLAG\_\*" are the associated wear/non-wear flags. It is worth noting that the wear/non-wear status was classified based on the count values.

## Load Data

```{r load data, message=FALSE}

library(tidyverse)
library(rnhanesdata)
library(caret)

```

Since all processed data have been stored in the `rnhanesdata` package, all we need to do is to load this package. We can also download raw data of other categories from [NHANES website](https://www.cdc.gov/nchs/nhanes/index.htm) and combine them with the processed data when needed.

## Check the Data Storage Format

Data collected from C and D waves were processed into exactly the same format, which helps to combine these two waves in the following analysis. This decision was made because these two waves share very similar protocol.

### Activity Count Data

```{r}
## check the storage format of activity count data
dim(PAXINTEN_C)
head(PAXINTEN_C[,1:10], n = 10)
table(PAXINTEN_C$WEEKDAY)
dim(PAXINTEN_D)
table(PAXINTEN_D$WEEKDAY)
```

There were 7176 participants from C wave and 7455 participants from D wave who wore the device, leading to a total of 14631 study participants. For each eligible study participant, we have 7 consecutive days of activity count data, which can start from any day of the week. Each day is defined as from midnight to midnight. The meaning of each column is explained below:

-   `SEQN`: Respondent sequence number (unique identifier).
-   `PAXCAL`: Monitor calibration indicator. `PAXCAL = 1` for calibrated data.
-   `PAXSTAT`: Data reliability status. `PAXSTAT = 1` for data deemed reliable.
-   `WEEKDAY`: Day of the week. `WEEKDAY = 1` for Sunday, `WEEKDAY = 2` for Monday, etc. The start date of wearing the device is not necessarily Sunday.
-   `SDDSRVYR`: Two-year data release cycle number. `SDDSRVYR = 3` for 2003-2004 ("C") wave, and `SDDSRVYR = 4` for 2005-2006 ("D") wave.
-   `MIN*`: Activity count value at each minute of a day. `MIN1`: 12:00AM-12:01AM, etc.

### Mortality Data

```{r}
## check the storage format of mortality data
dim(Mortality_2015_C)
head(Mortality_2015_C)
table(Mortality_2015_C$mortstat)
dim(Mortality_2015_D)
table(Mortality_2015_D$mortstat)
```

The number of rows of mortality data is the sample size of each wave. We have a total of 10122 participants enrolled in C wave and 10348 participants enrolled in D wave, respectively. Notice that not all study participants wore the device, which explains why we only have activity count data from around 71% of the population in each wave. In addition, not all study participants were linked to mortality data and the eligibility was labeled in the `eligstat` column. The meaning of each column is explained below:

-   `SEQN`: Respondent sequence number (unique identifier).
-   `eligstat`: Mortality linkage eligibility. `eligstat = 1` indicates that the survey participant was eligible for the mortality linkage.
-   `mortstat`: Vital status code. `mortstat = 1` if assumed deceased and `mortstat = 0` if assumed alive.
-   `permth_exm`: Person months of follow-up from MEC/Exam date.
-   `permth_int`: Person months of follow-up from interview date.
-   `ucod_leading`: Leading cause of death (code).
-   `diabetes_mcod`: Diabetes flag from multiple cause of death.
-   `hyperten_mcod`: Hypertension flag from multiple cause of death.

### Covariate Data

```{r}
## check the storage format of covariates
dim(Covariate_C)
head(Covariate_C)
dim(Covariate_D)
```

The number of rows of covariate data is the same as that of mortality data. For some variables we have missing values. The variables can be classified into several categories:

-   Demographic data: `SEQN`, `RIDAGEMN`, `RIDAGEEX`, `RIDAGEYR`, `BMI`, `BMI_cat`, `Race`, `Gender`, `EducationAdult`, `MobilityProblem`.
-   Survey design: `SDDSRVYR`, `SDMVPSU`, `SDMVSTRA`, `WTINT2YR`, `WTMEC2YR`.
-   Comorbidity: `Diabetes`, `CHF`, `CHD`, `Cancer`, `Stroke`.
-   Lifestyle: `DrinkStatus`, `DrinksPerWeek`, `SmokeCigs`.

For some demographic and survey design variables with unintuitive capitalized names, we can find their actual meaning on the NHANES website. The meaning of the other columns is easier to understand. It is worth noting that NHANES study has many other types of data that are not limited to those shown above. Other types of data can be downloaded from the website and integrated with existing data using the same processing pipeline.

## Data Cleaning

Since the purpose of this tutorial is to show **how to start** working with these large-scale, multilevel, high-dimensional, survey-weighted, and publicly available data, we only show necessary cleaning steps before combining these data into an analyzable format. For different projects, it is recommended to set your own data exclusion criteria and do further data cleaning accordingly.

```{r data cleaning, message=FALSE}
## load tidyverse package for data cleaning
library(tidyverse)

## change activity count value under non-wear flags to 0
PAXINTEN_C[,paste0("MIN",1:1440)] <- PAXINTEN_C[,paste0("MIN",1:1440)]*
  Flags_C[,paste0("MIN",1:1440)]
PAXINTEN_D[,paste0("MIN",1:1440)] <- PAXINTEN_D[,paste0("MIN",1:1440)]*
  Flags_D[,paste0("MIN",1:1440)]

## merge mortality and covariate data
mort_cov_C <- inner_join(Mortality_2015_C, Covariate_C, by = "SEQN")
mort_cov_D <- inner_join(Mortality_2015_D, Covariate_D, by = "SEQN")

## combine data collected from two waves
mort_cov <- bind_rows(mort_cov_C, mort_cov_D)
act_cnt <- bind_rows(PAXINTEN_C, PAXINTEN_D)
wear_flag <- bind_rows(Flags_C, Flags_D)
rm(mort_cov_C, mort_cov_D)

## create Age (in years) using the age at examination
mort_cov$Age <- mort_cov$RIDAGEEX/12
```

After these cleaning steps, there remains two major differences between the activity count data and the rest: (1) each row of activity count data is one participant-day, while each row of the other data is one participant; (2) each row of activity count data has minute-level values, which is high-dimensional. While such multilevel, high-dimensional structure itself leads to many interesting statistical research questions, in some other cases people prefer to have simple, participant-level summary measures to work with.

To solve challenge (2), we follow the literature and create several physical activity summary variables. To solve challenge (1), we compress the activity count data to participant-level by taking the average of (i) activity count value at each minute of a day, and (ii) summary variables, across "good days". A day is defined as an good day if: (i) it has estimated wear time of over 10 hours, (ii) the data are calibrated, and (iii) the data are deemed reliable by NHANES. We also only compress activity count data for participants with at least 3 good days. Notice that the purpose of such compression is to integrate activity count data with other participant-level data so that we have a single data frame with non-redundant information. In other words, not all of the following steps are necessary if we are only interested in the multilevel data. However, it is still recommended to use only "good days" in the multilevel data analysis.

### Further Cleaning of Activity Count Data

```{r}
## extract count values and flags as matrices
cnt_mat <- as.matrix(act_cnt[,paste0("MIN",1:1440)])
flag_mat <- as.matrix(wear_flag[,paste0("MIN",1:1440)])

## replace NAs with 0s
cnt_mat[is.na(cnt_mat)] <- 0
flag_mat[is.na(flag_mat)] <- 0

## calculate activity count summary measures
### total activity count (TAC)
act_cnt$TAC <- rowSums(cnt_mat)
### total log activity count (TLAC)
act_cnt$TLAC <- rowSums(log(1+cnt_mat))
### total wear time (WT)
act_cnt$WT <- rowSums(flag_mat)
### total sedentary time (ST)
act_cnt$ST <- rowSums(cnt_mat < 100) ## threshold set based on the literature
### total moderate to vigorous physical activity time (MVPA)
act_cnt$MVPA <- rowSums(cnt_mat >= 2020) ## threshold set based on the literature

## create "good day" indicator
act_cnt$goodday <- ifelse(act_cnt$PAXCAL == 1 & act_cnt$PAXSTAT == 1 & 
                            act_cnt$WT >= 600, 1, 0)

## store the minute-level activity count data as a column of the data frame
act_cnt$AC <- I(cnt_mat)

## clean the multilevel activity count data
act_cnt_ml <- act_cnt %>% filter(goodday == 1) %>%
  select("SEQN", "SDDSRVYR", "WEEKDAY",
         "AC", "TAC", "TLAC", "ST", "MVPA", "WT",
         "PAXCAL", "PAXSTAT", "goodday")

## add number of good days for each participant
act_cnt_ml <- left_join(act_cnt_ml, act_cnt_ml %>% count(SEQN) %>% 
                        mutate(n_good_days = n) %>% select(SEQN, n_good_days),
                        by = "SEQN")
dim(act_cnt_ml)
str(act_cnt_ml)

rm(act_cnt, wear_flag, cnt_mat, flag_mat)
```

The data frame `act_cnt_ml` contains cleaned multilevel activity count data in NHANES. This is the data we want to use for **multilevel** statistical modeling.

### Compression of Activity Count Data

```{r}
## select variables of activity count data to compress
act_cnt_ml2sl <- act_cnt_ml %>% filter(n_good_days >= 3) %>%
  select(SEQN, AC, TAC, TLAC, ST, MVPA, WT, n_good_days)

## compress activity count data into participant level
act_cnt_sl_summary <- aggregate(act_cnt_ml2sl[,3:ncol(act_cnt_ml2sl)], 
                        list(SEQN = act_cnt_ml2sl$SEQN), mean)
### for the count value matrix stored in "AC" column, we have to do aggregation manually
inx_row <- split(1:nrow(act_cnt_ml2sl), f = factor(act_cnt_ml2sl$SEQN))
act_cnt_sl_tseries <- I(t(vapply(inx_row, function(x) 
                   colMeans(act_cnt_ml2sl$AC[x,,drop=FALSE],na.rm=TRUE),
                   numeric(ncol(act_cnt_ml2sl$AC)))))

dim(act_cnt_sl_summary)
str(act_cnt_sl_summary)
dim(act_cnt_sl_tseries)

rm(act_cnt_ml, act_cnt_ml2sl, inx_row)
```

# Filtering down to subjects in wave D with quality controlled AC data and mortality outcome available

```{r}

# IDs of all subs in wave D
D_ids <- unique(Mortality_2015_D$SEQN) 

# IDs of subs in wave D with quality control passing activity data
activity_ids <- act_cnt_sl_summary %>%
  filter(SEQN %in% D_ids) %>%
  pull(SEQN)

# Filter to subs in wave D with mortality data & activity data
mort_cov_sample <- mort_cov %>%
  filter(SEQN %in% D_ids) %>% 
  filter(eligstat==1 & SEQN %in% activity_ids)

# Extract sample IDs
sample_ids <- mort_cov_sample %>% pull(SEQN)

# Filter activity data to sample IDs
activity_index <- which(act_cnt_sl_summary$SEQN %in% sample_ids)
act_cnt_sl_summary_sample <- act_cnt_sl_summary[activity_index, ]
act_cnt_sl_tseries_sample <- act_cnt_sl_tseries[activity_index, ]

rm(act_cnt_sl_summary, mort_cov, act_cnt_sl_tseries, activity_ids, activity_index, D_ids)

```

# Binning data into 6 minute bins

Here we bucket the single-level activity into 6 minute bins:

```{r}

# Removing the AsIs class attribute manually
class(act_cnt_sl_tseries_sample) <- setdiff(class(act_cnt_sl_tseries_sample), "AsIs")
# Optionally convert to matrix if it's still not a matrix
if (!is.matrix(act_cnt_sl_tseries_sample)) {
  act_cnt_sl_tseries_sample <- as.matrix(act_cnt_sl_tseries_sample)
}
# Check the class of the matrix
class(act_cnt_sl_tseries_sample)
# Convert to tibble and add observation ID
AC_df <- act_cnt_sl_tseries_sample %>% 
  as.tibble() %>%
  mutate(SEQN = act_cnt_sl_summary_sample$SEQN)
# Melt into a long format
AC_long <- AC_df %>% 
  pivot_longer(
    -SEQN, 
    names_to = "minute",
    values_to = "activity_level",
    names_prefix = "minute_"
  ) %>%
  mutate(minute = str_remove(minute, "MIN") %>% as.integer()) %>%
  mutate(bin_index = (minute - 1) %/% 6 + 1)
# Calculate the mean for each bin
AC_binned <- AC_long %>%
  group_by(SEQN, bin_index) %>%
  summarise(
    avg_activity = mean(activity_level, na.rm = TRUE)
  ) %>%
  ungroup()

# If needed, pivot back to a wide format
AC_wide <- AC_binned %>%
  pivot_wider(
    names_from = bin_index,
    values_from = avg_activity,
    names_prefix = "bin_"
  )

# AC_wide is what is wanted, so we remove intermediate objects
rm(act_cnt_sl_tseries_sample, AC_df, AC_long, AC_binned)

```

# Split into train, valid, and test sets

The data we want to include in our analyses are contained in the `AC_wide` and `mort_cov_sample` objects, so we need to split these and store them in a form useful for conversion into mtf images and use in CNN-based modeling.

```{r}

set.seed(1251)

# Create indices for the training set (70% of the data)
train_indices <- createDataPartition(sample_ids, p = 0.70, list = FALSE, times = 1)

# Subset the sample_ids to create the training set
train_set <- sample_ids[train_indices]

# For validation and test sets, you need to split the remaining 30%
remaining_ids <- sample_ids[-train_indices]

# Create indices for the validation set (50% of the remaining, which is 15% of total)
validate_indices <- createDataPartition(remaining_ids, p = 0.50, list = FALSE, times = 1)

# Subset the remaining_ids for validation set
validate_set <- remaining_ids[validate_indices]

# The rest goes to the test set
test_set <- remaining_ids[-validate_indices]

# Output the sets
DataPartitions <- list(train = train_set, validate = validate_set, test = test_set)

# Numbers in each set
set_size <- sapply(DataPartitions, length)
print(set_size)

# Make csv
set_lookup <- data.frame(
  SEQN=c(DataPartitions$train,
         DataPartitions$validate,
         DataPartitions$test),
  set=c(rep("train", set_size[1]),
        rep("valid", set_size[2]),
        rep("test", set_size[3]))
)

# Store sets
write.csv(set_lookup, file = "set_lookup.csv")

```

We re-split our single-level activity data into waves C and D to store them for translation to images by the Markov Transition Field method.

```{r}

# Splitting AC_wide
AC <- list()
AC$train <- AC_wide[AC_wide$SEQN %in% DataPartitions$train, ]
AC$valid <- AC_wide[AC_wide$SEQN %in% DataPartitions$valid, ]
AC$test <- AC_wide[AC_wide$SEQN %in% DataPartitions$test, ]

# Splitting mort_cov_sample
mort_cov <- list()
mort_cov$train <- mort_cov_sample[mort_cov_sample$SEQN %in% 
                              DataPartitions$train, ]
mort_cov$valid <- mort_cov_sample[mort_cov_sample$SEQN %in% 
                              DataPartitions$valid, ]
mort_cov$test <- mort_cov_sample[mort_cov_sample$SEQN %in% 
                             DataPartitions$test, ]

# Store our sample's data for further processing
split_names <- names(AC)
for (i in 1:length(split_names)) {
  write.csv(mort_cov[[i]], 
            paste0("data/mort_cov_", 
                   split_names[i], ".csv"))
  write.csv(AC[[i]], 
            paste0("data/AC_", 
                   split_names[i], ".csv"))
}

```
